{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_FILE = 'SpamData/01_Processing/practice_email.txt'\n",
    "\n",
    "SPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_1'\n",
    "SPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_2'\n",
    "EASY_NONSPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_1'\n",
    "EASY_NONSPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_2'\n",
    "\n",
    "SPAM_CAT = 1\n",
    "HAM_CAT = 0\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DATA_JSON_FILE = 'SpamData/01_Processing/email-text-data.json'\n",
    "WORD_ID_FILE = 'SpamData/01_Processing/word-by-id.csv'\n",
    "\n",
    "TRAINING_DATA_FILE = 'SpamData/02_Training/train-data.txt'\n",
    "TEST_DATA_FILE = 'SpamData/02_Training/test-data.txt'\n",
    "\n",
    "WHALE_FILE = 'SpamData/01_Processing/wordcloud_resources/whale-icon.png'\n",
    "SKULL_FILE = 'SpamData/01_Processing/wordcloud_resources/skull-icon.png'\n",
    "THUMBS_UP_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-up.png'\n",
    "THUMBS_DOWN_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-down.png'\n",
    "CUSTOM_FONT_FILE = 'SpamData/01_Processing/wordcloud_resources/OpenSansCondensed-Bold.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding='latin-1')\n",
    "message = stream.read()\n",
    "stream.close()\n",
    "\n",
    "print(type(message))\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getfilesystemencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding='latin-1')\n",
    "\n",
    "is_body = False\n",
    "lines = []\n",
    "\n",
    "for line in stream:\n",
    "    if is_body:\n",
    "        lines.append(line)\n",
    "    elif line == '\\n':\n",
    "        is_body = True\n",
    "\n",
    "stream.close()\n",
    "\n",
    "email_body = '\\n'.join(lines)\n",
    "print(email_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squares(N):\n",
    "    for my_number in range(N):\n",
    "        yield my_number ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in generate_squares(5):\n",
    "    print(i, end=' ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Email body extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_generator(path):\n",
    "    \n",
    "    for root, dirnames, filenames in walk(path):\n",
    "        for file_name in filenames:\n",
    "            \n",
    "            filepath = join(root, file_name)\n",
    "            \n",
    "            stream = open(filepath, encoding='latin-1')\n",
    "\n",
    "            is_body = False\n",
    "            lines = []\n",
    "\n",
    "            for line in stream:\n",
    "                if is_body:\n",
    "                    lines.append(line)\n",
    "                elif line == '\\n':\n",
    "                    is_body = True\n",
    "\n",
    "            stream.close()\n",
    "\n",
    "            email_body = '\\n'.join(lines)\n",
    "            \n",
    "            yield file_name, email_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_directory(path, classification):\n",
    "    rows = []\n",
    "    row_names = []\n",
    "    \n",
    "    for file_name, email_body in email_body_generator(path):\n",
    "        rows.append({'MESSAGE': email_body, 'CATEGORY': classification})\n",
    "        row_names.append(file_name)\n",
    "        \n",
    "    return pd.DataFrame(rows, index=row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = df_from_directory(SPAM_1_PATH, 1)\n",
    "spam_emails = spam_emails.append(df_from_directory(SPAM_2_PATH, 1))\n",
    "spam_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = df_from_directory(EASY_NONSPAM_1_PATH, HAM_CAT)\n",
    "ham_emails = ham_emails.append(df_from_directory(EASY_NONSPAM_2_PATH, HAM_CAT))\n",
    "ham_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([spam_emails, ham_emails])\n",
    "print('Shape of entire dataframe is ', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any message bodies are null\n",
    "data['MESSAGE'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are empty emails (string length zero)\n",
    "(data.MESSAGE.str.len() == 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.MESSAGE.str.len() == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: how would you check the number of entries with null/None values?\n",
    "data.MESSAGE.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate empty emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.MESSAGE.str.len() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.MESSAGE.str.len() == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.get_loc('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4608:4611]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove System File Entries from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['cmds', '.DS_Store'], inplace=True)\n",
    "\n",
    "data[4608:4611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Document IDs to Track Emails in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = range(0, len(data.index))\n",
    "data['DOC_ID'] = document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['FILE_NAME'] = data.index\n",
    "data.set_index('DOC_ID', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to File using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(DATA_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Spam Messages Visualised (Pie Charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_spam = data.CATEGORY.value_counts()[1]\n",
    "amount_of_ham = data.CATEGORY.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amount_of_spam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m category_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLegit Mail\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m sizes \u001b[38;5;241m=\u001b[39m [\u001b[43mamount_of_spam\u001b[49m, amount_of_ham]\n\u001b[0;32m      3\u001b[0m custom_colours \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ff7675\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#74b9ff\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m227\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'amount_of_spam' is not defined"
     ]
    }
   ],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colours = ['#ff7675', '#74b9ff']\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, explode=[0, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colours = ['#ff7675', '#74b9ff']\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, pctdistance=0.8)\n",
    "\n",
    "# draw circle\n",
    "centre_circle = plt.Circle((0, 0), radius=0.6, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail', 'Updates', 'Promotions']\n",
    "sizes = [25, 43, 19, 22]\n",
    "custom_colours = ['#ff7675', '#74b9ff', '#55efc4', '#ffeaa7']\n",
    "offset = [0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, pctdistance=0.8, explode=offset)\n",
    "\n",
    "# draw circle\n",
    "centre_circle = plt.Circle((0, 0), radius=0.6, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "msg = 'All work and no play makes Jack a dull boy.'\n",
    "msg.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the NLTK Resources (Tokenizer & Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy.'\n",
    "word_tokenize(msg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'this' in stop_words: print('Found it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: print out 'Nope. Not in here' if the word \"hello\" is not contained in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hello' not in stop_words: print('Nope. Not in here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be.'\n",
    "words = word_tokenize(msg.lower())\n",
    "\n",
    "filtered_words = []\n",
    "# Challenge: append non-stop words to filtered_words\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Stems and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be. \\\n",
    "      Nobody expects the Spanish Inquisition!'\n",
    "words = word_tokenize(msg.lower())\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "filtered_words = []\n",
    "# Challenge: append non-stop words to filtered_words\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'p'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'?'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be. ??? \\\n",
    "      Nobody expects the Spanish Inquisition!'\n",
    "\n",
    "words = word_tokenize(msg.lower())\n",
    "stemmer = SnowballStemmer('english')\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words and word.isalpha():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML tags from Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data.at[2, 'MESSAGE'], 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Email Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message, stemmer=PorterStemmer(), \n",
    "                 stop_words=set(stopwords.words('english'))):\n",
    "    \n",
    "    # Converts to Lower Case and splits up the words\n",
    "    words = word_tokenize(message.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_message(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Modify function to remove HTML tags. Then test on Email with DOC_ID 2. \n",
    "def clean_msg_no_html(message, stemmer=PorterStemmer(), \n",
    "                 stop_words=set(stopwords.words('english'))):\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(message, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    \n",
    "    # Converts to Lower Case and splits up the words\n",
    "    words = word_tokenize(cleaned_text.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "#             filtered_words.append(word) \n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[2, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Cleaning and Tokenisation to all messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Dataframes and Series & Creating Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iat[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_emails = data.MESSAGE.iloc[0:3]\n",
    "\n",
    "nested_list = first_emails.apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = []\n",
    "# for sublist in nested_list:\n",
    "#     for item in sublist:\n",
    "#         flat_list.append(item)\n",
    "\n",
    "flat_list = [item for sublist in nested_list for item in sublist]\n",
    "        \n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# use apply() on all the messages in the dataframe\n",
    "nested_list = data.MESSAGE.apply(clean_msg_no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logic to Slice Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.CATEGORY == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.CATEGORY == 1].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: create two variables (doc_ids_spam, doc_ids_ham) which \n",
    "# hold onto the indices for the spam and the non-spam emails respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_spam = data[data.CATEGORY == 1].index\n",
    "doc_ids_ham = data[data.CATEGORY == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting a Series with an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc_ids_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham = nested_list.loc[doc_ids_ham]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_spam = nested_list.loc[doc_ids_spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: use python list comprehension and then find the total number of \n",
    "# words in our cleaned dataset of spam email bodies. Also find the total number of \n",
    "# words in normal emails in the dataset. Then find the 10 most common words used in \n",
    "# spam. Also, find the 10 most common words used in non-spam messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_ham = [item for sublist in nested_list_ham for item in sublist]\n",
    "normal_words = pd.Series(flat_list_ham).value_counts()\n",
    "\n",
    "normal_words.shape[0] # total number of unique words in the non-spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_spam = [item for sublist in nested_list_spam for item in sublist]\n",
    "spammy_words = pd.Series(flat_list_spam).value_counts()\n",
    "\n",
    "spammy_words.shape[0] # total number of unique words in the spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammy_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate(email_body)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "len(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [''.join(word) for word in example_corpus]\n",
    "novel_as_string = ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(WHALE_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', \n",
    "                      max_words=400, colormap='ocean')\n",
    "\n",
    "word_cloud.generate(novel_as_string)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array[1023, 2047]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array[500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: use the skull image in the lesson resources to create a word cloud\n",
    "# for Shakespeare's play Hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_corpus = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "word_list = [''.join(word) for word in hamlet_corpus]\n",
    "hamlet_as_string = ' '.join(word_list)\n",
    "\n",
    "skull_icon = Image.open(SKULL_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=skull_icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(skull_icon, box=skull_icon)\n",
    "rgb_array = np.array(image_mask)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white',\n",
    "                      colormap='bone', max_words=600)\n",
    "\n",
    "word_cloud.generate(hamlet_as_string)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud of Ham and Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_UP_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "# Generate the text as a string for the word cloud\n",
    "ham_str = ' '.join(flat_list_ham)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', \n",
    "                      max_words=500, colormap='winter')\n",
    "\n",
    "word_cloud.generate(ham_str)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Look at the word cloud documentation. Use the custom font included in the \n",
    "# lesson resources instead of the default font and create a word cloud of the spammy words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_DOWN_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "# Generate the text as a string for the word cloud\n",
    "spam_str = ' '.join(flat_list_spam)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_font_size=300,\n",
    "                      max_words=2000, colormap='gist_heat', font_path=CUSTOM_FONT_FILE)\n",
    "\n",
    "word_cloud.generate(spam_str.upper())\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Vocabulary & Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_nested_list = data.MESSAGE.apply(clean_msg_no_html)\n",
    "flat_stemmed_list = [item for sublist in stemmed_nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.Series(flat_stemmed_list).value_counts()\n",
    "print('Nr of unique words', unique_words.shape[0])\n",
    "unique_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Create subset of the series called 'frequent_words' that only contains\n",
    "# the most common 2,500 words out of the total. Print out the top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = unique_words[0:VOCAB_SIZE]\n",
    "print('Most common words: \\n', frequent_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary DataFrame with a WORD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = list(range(0, VOCAB_SIZE))\n",
    "vocab = pd.DataFrame({'VOCAB_WORD': frequent_words.index.values}, index=word_ids)\n",
    "vocab.index.name = 'WORD_ID'\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Vocabulary as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv(WORD_ID_FILE, index_label=vocab.index.name, header=vocab.VOCAB_WORD.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Checking if a Word is Part of the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Write a line of code that checks if a particular word is part \n",
    "# of the vocabulary. Your code should return True if the word is among the \n",
    "# 2,500 words that comprise the vocabulary, and False otherwise. Check these words:\n",
    "# 'machine'\n",
    "# 'learning'\n",
    "# 'fun'\n",
    "# 'learn'\n",
    "# 'data'\n",
    "# 'science'\n",
    "# 'app'\n",
    "# 'brewery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(vocab.VOCAB_WORD == 'machine') # inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'brew' in set(vocab.VOCAB_WORD) # better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Find the Email with the Most Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Print out the number of words in the longest email (after cleaning & stemming).\n",
    "# Note the longest email's position in the list of cleaned emails. Print out the stemmed\n",
    "# list of words in the longest email. Print out the longest email from the data dataframe.\n",
    "\n",
    "# Hint: use the len() function and practice list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop\n",
    "clean_email_lengths = []\n",
    "for sublist in stemmed_nested_list:\n",
    "    clean_email_lengths.append(len(sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python List Comprehension\n",
    "clean_email_lengths = [len(sublist) for sublist in stemmed_nested_list]\n",
    "print('Nr words in the longest email:', max(clean_email_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email position in the list (and the data dataframe)', np.argmax(clean_email_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_nested_list[np.argmax(clean_email_lengths)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[np.argmax(clean_email_lengths), 'MESSAGE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features & a Sparse Matrix\n",
    "\n",
    "### Creating a DataFrame with one Word per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_columns_df = pd.DataFrame.from_records(stemmed_nested_list.tolist())\n",
    "word_columns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_columns_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into a Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Can you split the data into a training and testing set? Set the test size at 30%. \n",
    "# The training data should include 4057 emails. Use a seed value of 42 to shuffle the data. \n",
    "# What should the target values be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_columns_df, data.CATEGORY,\n",
    "                                                   test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nr of training samples', X_train.shape[0])\n",
    "print('Fraction of training set', X_train.shape[0] / word_columns_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.name = X_test.index.name = 'DOC_ID'\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create a Sparse Matrix for the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = pd.Index(vocab.VOCAB_WORD)\n",
    "type(word_index[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index.get_loc('thu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, indexed_words, labels):\n",
    "    \"\"\"\n",
    "    Returns sparse matrix as dataframe.\n",
    "    \n",
    "    df: A dataframe with words in the columns with a document id as an index (X_train or X_test)\n",
    "    indexed_words: index of words ordered by word id\n",
    "    labels: category as a series (y_train or y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                category = labels.at[doc_id]\n",
    "                \n",
    "                item = {'LABEL': category, 'DOC_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "    \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_train_df = make_sparse_matrix(X_train, word_index, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Combine Occurrences with the Pandas groupby() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = sparse_train_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum()\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.at[0, 'VOCAB_WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = train_grouped.reset_index()\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.at[1923, 'VOCAB_WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[5795]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training Data as .txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRAINING_DATA_FILE, train_grouped, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Can you create a sparse matrix for the test data. Group the occurrences of the same word in the same email. Then save the data as a .txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_test_df = make_sparse_matrix(X_test, word_index, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grouped = sparse_test_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum().reset_index()\n",
    "test_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TEST_DATA_FILE, test_grouped, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pre-Processing Subtleties and Checking your Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: We started with 5796 emails. We split it into 4057 emails for training and 1739 emails for testing. \n",
    "\n",
    "How many individual emails were included in the testing .txt file? Count the number in the test_grouped DataFrame. After splitting and shuffling our data, how many emails were included in the X_test DataFrame? Is the number the same? If not, which emails were excluded and why? Compare the DOC_ID values to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ids = set(train_grouped.DOC_ID)\n",
    "test_doc_ids = set(test_grouped.DOC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_test.index.values) - test_doc_ids # Excluded emails after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[14, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[1096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[1096, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message(data.at[1096, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
